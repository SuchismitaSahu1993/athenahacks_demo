{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Classification\n",
    "\n",
    "Workflow for supervised learning on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ignore warning messages (sklearn has a ton)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Office df shape: (41467, 6)\n",
      "Overwatch df shape: (47774, 6)\n"
     ]
    }
   ],
   "source": [
    "# combine all daasets\n",
    "office_df = pd.read_csv('../data/dundermifflin.csv')\n",
    "print('Office df shape:', office_df.shape)\n",
    "\n",
    "overwatch_df = pd.read_csv('../data/overwatch.csv')\n",
    "print('Overwatch df shape:', overwatch_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89241, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I read somewhere that most people who think th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got Oscar Martinez... Michael am I gay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That is correct.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Am I the only one who took slight pride in get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You got: Creed Bratton\\nYou're very mysterious...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       title      id      subreddit body  \\\n",
       "0           0  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "1           1  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "2           2  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "3           3  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "4           4  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "\n",
       "                                             comment  \n",
       "0  I read somewhere that most people who think th...  \n",
       "1          I got Oscar Martinez... Michael am I gay?  \n",
       "2                                   That is correct.  \n",
       "3  Am I the only one who took slight pride in get...  \n",
       "4  You got: Creed Bratton\\nYou're very mysterious...  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine data into single DataFrame\n",
    "all_df = pd.concat([office_df, overwatch_df])\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overwatch        47774\n",
       "DunderMifflin    41467\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data looks fairly balanced\n",
    "all_df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text\n",
    "\n",
    "As a first step, we'll tokenize the documents, remove stopwords, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim requires that we create a `gensim.corpora.Dictionary` object for our text.\n",
    "This object expects a list of lists, (each nested list itself a list of tokens), so we'll ensure our preprocessing functions output our data as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
    "\n",
    "# Unix System (mac/ubuntu/etc.)\n",
    "# nlp = spacy.load('en', disable=['ner', 'parser']) \n",
    "\n",
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# you can also just use your own stopwords\n",
    "stop_words.update(['los', 'angeles'])\n",
    "\n",
    "def clean_token(token):\n",
    "    c_token = re.sub(\"[^A-Za-z']+\", ' ', str(token))\n",
    "    # lower-case and strip whitespace\n",
    "    c_token = c_token.lower().strip()\n",
    "    # remove stopwords\n",
    "    if c_token in stop_words:\n",
    "        return ''\n",
    "    return c_token\n",
    "\n",
    "def clean_comment(comment):\n",
    "    if not isinstance(comment, str):\n",
    "        return ['']\n",
    "    clean_tokens = [\n",
    "        clean_token(token) \n",
    "        for token in comment.split()\n",
    "    ]\n",
    "    # remove empty strings\n",
    "    cleaned_tokens = [com for com in clean_tokens \n",
    "                      if com != '']\n",
    "    cleaned_comment = ' '.join(cleaned_tokens)\n",
    "    return cleaned_comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usc cool\n"
     ]
    }
   ],
   "source": [
    "example_text = \"USC is cool but Los Angeles is whatever.\"\n",
    "print(clean_comment(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text-cleaning to all comment columns\n",
    "all_df['clean_comment'] = all_df['comment'].apply(lambda x: clean_comment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                       got oscar martinez michael gay\n",
       "2                                              correct\n",
       "3                    took slight pride getting michael\n",
       "4    got creed bratton you're mysterious people har...\n",
       "5    damn took test i'm phyllis guess lot learn tow...\n",
       "6    god took quiz time im toby camera document rea...\n",
       "7                                 identity theft crime\n",
       "8                                          daryll love\n",
       "9    quiz dwight retaking order michael you're defi...\n",
       "Name: clean_comment, dtype: object"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['clean_comment'][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training/Testing Sets\n",
    "\n",
    "Notes, it's something of a convention to use `X` for the dataset, and `y` for the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = all_df['clean_comment']\n",
    "y = all_df['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (62468,)\n",
      "Testing Data Shape: (26773,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "Before throwing our into ml models, we need to create embeddings.\n",
    "This can be done in several ways, with complexity differening depending on the desired method.\n",
    "For this notebook, we'll keep it simple and just use scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf embeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# init model w/ default params\n",
    "tfidf  = TfidfVectorizer()\n",
    "\n",
    "# fit transformer to our data\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "# transform our data\n",
    "corpus_train = tfidf.transform(X_train)\n",
    "\n",
    "# corpus_train = tfidf.fit_transform(X_train)\n",
    "corpus_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (62468, 33761)\n",
      "Testing Shape: (26773, 33761)\n"
     ]
    }
   ],
   "source": [
    "print('Training Shape:', corpus_train.shape)\n",
    "print('Testing Shape:', corpus_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn Estimators\n",
    "\n",
    "Using an `sklearn` ml algorithm is super simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77223322003511"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# init classifier with default params\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# train classifier on our data\n",
    "rf.fit(corpus_train, y_train)\n",
    "\n",
    "# use classifier to make predictions\n",
    "y_preds = rf.predict(corpus_test)\n",
    "\n",
    "# get accuracy\n",
    "accuracy_score(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn pipelines\n",
    "\n",
    "Idea: throw everything together into one object. Treat this object itself as a single classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111903783662645"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Simple Pipeline\n",
    "pipeline_model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "pipeline_model.fit(X_train, y_train)\n",
    "y_preds = pipeline_model.predict(X_test)\n",
    "accuracy_score(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycling through multiple embeddings + pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a pipeline allows us to iterate through many different model + vector combinations.\n",
    "Also, the code becomes more concise, and more expressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def create_pipeline(vectorizer, estimator, reducer=False):\n",
    "    \"\"\"\n",
    "    Create pipeline with optional dimensionality-reduction.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('vectorizer', vectorizer)\n",
    "    ]\n",
    "    if reducer:\n",
    "        steps.append(('reducer', TruncatedSVD()))\n",
    "    steps.append(('classifier', estimator))\n",
    "    return Pipeline(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8014417510178165\n"
     ]
    }
   ],
   "source": [
    "# Single Classifier with pipeline\n",
    "\n",
    "pipe = create_pipeline(CountVectorizer(), SGDClassifier(), reducer=False)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_preds = pipe.predict(X_test)\n",
    "print(accuracy_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "       ...penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]), Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "       ...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))]), Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "       ...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))])]\n"
     ]
    }
   ],
   "source": [
    "# Create combinations\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "for vectorizer in (CountVectorizer(), TfidfVectorizer()):\n",
    "    for estimator in (LogisticRegression, SGDClassifier, RandomForestClassifier):\n",
    "        models.append(create_pipeline(vectorizer, estimator(), reducer=False))\n",
    "        models.append(create_pipeline(vectorizer, estimator(), reducer=True))\n",
    "\n",
    "print(models[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression'> without dimensionality reduction: 0.8111903783662645\n",
      "Accuracy of LogisticRegression'> with dimensionality reduction: 0.5318791319613043\n",
      "Accuracy of SGDClassifier'> without dimensionality reduction: 0.7990512830090016\n",
      "Accuracy of SGDClassifier'> with dimensionality reduction: 0.5318791319613043\n",
      "Accuracy of RandomForestClassifier'> without dimensionality reduction: 0.7696559967131065\n",
      "Accuracy of RandomForestClassifier'> with dimensionality reduction: 0.5620587905725918\n",
      "Accuracy of LogisticRegression'> without dimensionality reduction: 0.809584282672842\n",
      "Accuracy of LogisticRegression'> with dimensionality reduction: 0.5336346319052777\n",
      "Accuracy of SGDClassifier'> without dimensionality reduction: 0.793560676801255\n",
      "Accuracy of SGDClassifier'> with dimensionality reduction: 0.5318791319613043\n",
      "Accuracy of RandomForestClassifier'> without dimensionality reduction: 0.7727187838494005\n",
      "Accuracy of RandomForestClassifier'> with dimensionality reduction: 0.5268367385052105\n"
     ]
    }
   ],
   "source": [
    "# Assess Models\n",
    "scores = []\n",
    "for model in models:\n",
    "    model_name = str(type(model.named_steps['classifier'])).split('.')[-1]\n",
    "    \n",
    "    if 'reducer' in model.named_steps:\n",
    "        acc_print = 'Accuracy of {} with dimensionality reduction: {}'\n",
    "    else:\n",
    "        acc_print = 'Accuracy of {} without dimensionality reduction: {}'\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores.append(accuracy)\n",
    "    print(acc_print.format(model_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
