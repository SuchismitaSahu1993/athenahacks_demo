{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Classification\n",
    "\n",
    "Workflow for supervised learning on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ignore warning messages (sklearn has a ton)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all daasets\n",
    "usc_df = pd.read_csv('../data/usc.csv')\n",
    "stats_df = pd.read_csv('../data/statistics.csv')\n",
    "office_df = pd.read_csv('../data/dundermifflin.csv')\n",
    "overwatch_df = pd.read_csv('../data/overwatch.csv')\n",
    "dutch_df = pd.read_csv('../data/dutch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([office_df, overwatch_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89241, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I read somewhere that most people who think th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got Oscar Martinez... Michael am I gay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That is correct.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Am I the only one who took slight pride in get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You got: Creed Bratton\\nYou're very mysterious...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       title      id      subreddit body  \\\n",
       "0           0  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "1           1  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "2           2  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "3           3  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "4           4  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "\n",
       "                                             comment  \n",
       "0  I read somewhere that most people who think th...  \n",
       "1          I got Oscar Martinez... Michael am I gay?  \n",
       "2                                   That is correct.  \n",
       "3  Am I the only one who took slight pride in get...  \n",
       "4  You got: Creed Bratton\\nYou're very mysterious...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Overwatch        47774\n",
       "DunderMifflin    41467\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text\n",
    "\n",
    "As a first step, we'll tokenize the documents, remove stopwords, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim requires that we create a `gensim.corpora.Dictionary` object for our text.\n",
    "This object expects a list of lists, (each nested list itself a list of tokens), so we'll ensure our preprocessing functions output our data as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_token(token):\n",
    "    c_token = re.sub(\"[^A-Za-z']+\", ' ', str(token))\n",
    "    # lower-case and strip whitespace\n",
    "    c_token = c_token.lower().strip()\n",
    "    # remove stopwords\n",
    "    if c_token in stop_words:\n",
    "        return ''\n",
    "    return c_token\n",
    "\n",
    "def clean_comment(comment):\n",
    "    if not isinstance(comment, str):\n",
    "        return ['']\n",
    "    cleaned_comment = [\n",
    "        clean_token(token) \n",
    "        for token in comment.split()\n",
    "    ]\n",
    "    # remove non-empty strings\n",
    "#     cleaned_comment = [com for com in cleaned_comment \n",
    "#                       if com != '']\n",
    "    return cleaned_comment\n",
    "\n",
    "stop_words = ['fuck', 'hey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['clean_comment'] = all_df['comment'].apply(lambda x: ' '.join(clean_comment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i read somewhere that most people who think th...\n",
       "1                    i got oscar martinez michael am i gay\n",
       "2                                          that is correct\n",
       "3        am i the only one who took slight pride in get...\n",
       "4        you got creed bratton you're very mysterious a...\n",
       "5        damn i just took a test and i'm a phyllis i gu...\n",
       "6        god i just took the quiz for the first time ev...\n",
       "7                                identity theft is a crime\n",
       "8                                          daryll  love it\n",
       "9        if you take a quiz and get dwight and keep ret...\n",
       "10                                                 or ryan\n",
       "11       i just took it and apparently i m pam makes sense\n",
       "12             lmao this  year old woman is stanley hudson\n",
       "13                                     apparently i m andy\n",
       "14       you re even more of a michael if you keep reta...\n",
       "15       apparently i'm a meredith i'm not sure how i f...\n",
       "16       if you take a what character from the office a...\n",
       "17       got jim but i think i m andy got i would hate ...\n",
       "18       i see no one else will admit to having got angela\n",
       "19                           i got tobby now i want to die\n",
       "20       i feel like michael is retaking the quiz until...\n",
       "21                                         yay i m a pammy\n",
       "22       i clicked the link took the quiz and the resul...\n",
       "23                                             i got creed\n",
       "24           no michael would retake it until he gets ryan\n",
       "25                      i got oscar just found out i m gay\n",
       "26                  guys help i took the quiz and got ryan\n",
       "27              my cat's name is tgymothy the t is silentl\n",
       "28       anyone who takes an online personality test is...\n",
       "29                   am i the only one here who got kelly \n",
       "                               ...                        \n",
       "47744    me too it feels cheap and too easy best games ...\n",
       "47745    just makes me mad not only is it hurting the o...\n",
       "47746    that wouldn't change anything no matter what s...\n",
       "47747    have you heard about this game called team for...\n",
       "47748    had a game where i was the solo zen on hanamur...\n",
       "47749      your compliment is appreciated harry potter fan\n",
       "47750        ehh mate just hold on until you're a decadian\n",
       "47751    this feels like what i tell myself since i'm s...\n",
       "47752    anybody can become grandmaster if they spend m...\n",
       "47753    i'm pretty sure jeff already stated that the n...\n",
       "47754    rocket league is coming that alone may get me ...\n",
       "47755                   well monster hunter had a good run\n",
       "47756    skyrim on the switch is pretty fun and thats t...\n",
       "47757                                         snipperclips\n",
       "47758    how exactly are we nearing a merger when a con...\n",
       "47759    then doing a shit port long after the hype die...\n",
       "47760    well the avoid player did get abused high rank...\n",
       "47761    they confirmed that they're doing a random red...\n",
       "47762                                     flair checks out\n",
       "47763    yeahhh i tend to do this when i'm not playing ...\n",
       "47764    team based shooters aren't as black and white ...\n",
       "47765    yeah we had private server communities which n...\n",
       "47766    bravo i love playing zen for some reason maybe...\n",
       "47767                                              deleted\n",
       "47768               rocket league on the go sounds awesome\n",
       "47769    i'm gonna be honest as a switch owner skyrim d...\n",
       "47770    i understand that but if you can abuse one you...\n",
       "47771    there is no honor on ground scatters either i ...\n",
       "47772    the way d va works you'd better have a high k ...\n",
       "47773    pretty much they at least used to chatban peop...\n",
       "Name: clean_comment, Length: 89241, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['clean_comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "Before throwing our into ml models, we need to create embeddings.\n",
    "This can be done in several ways, with complexity differening depending on the desired metod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from gensim.sklearn_api import TfIdfTransformer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from gensim.sklearn_api import LdaSeqTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_label(x):\n",
    "#     if x == 'Overwatch':\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# all_df['label'] = all_df['subreddit'].apply(lambda x: make_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df['com'] = all_df['clean_comment'].apply(lambda x: ' '.join(x))\n",
    "X = all_df['clean_comment']\n",
    "y = all_df['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# init model w/ default params\n",
    "tfidf  = TfidfVectorizer()\n",
    "\n",
    "corpus_train = tfidf.fit_transform(X_train)\n",
    "corpus_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# select classifier, and fit\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612146565569791"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model and view accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_preds = rf.predict(corpus_test)\n",
    "accuracy_score(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn pipelines\n",
    "\n",
    "Idea: throw everything together into one object. Treat this object itself as a single classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def create_pipeline(vectorizer, estimator, reducer=False):\n",
    "    \"\"\"\n",
    "    Create pipeline with optional dimensionality-reduction.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('vectorizer', vectorizer)\n",
    "    ]\n",
    "    if reducer:\n",
    "        steps.append(('reducer', TruncatedSVD()))\n",
    "    steps.append(('classifier', estimator))\n",
    "    return Pipeline(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7927389534232249\n"
     ]
    }
   ],
   "source": [
    "pipe = create_pipeline(CountVectorizer(), SGDClassifier(), reducer=False)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_preds = pipe.predict(X_test)\n",
    "print(accuracy_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a pipeline allows us to iterate through many different model + vector combinations.\n",
    "Also, the code becomes more concise, and more expressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "       ...penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]), Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "       ...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))]), Pipeline(memory=None,\n",
      "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "       ...m_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False))])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "for vectorizer in (CountVectorizer(), TfidfVectorizer()):\n",
    "    for estimator in (LogisticRegression, SGDClassifier, RandomForestClassifier):\n",
    "        models.append(create_pipeline(vectorizer, estimator(), reducer=False))\n",
    "        models.append(create_pipeline(vectorizer, estimator(), reducer=True))\n",
    "\n",
    "print(models[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression'> without dimensionality reduction: 0.8130952825607889\n",
      "Accuracy of LogisticRegression'> with dimensionality reduction: 0.5350913233481492\n",
      "Accuracy of SGDClassifier'> without dimensionality reduction: 0.792440144922123\n",
      "Accuracy of SGDClassifier'> with dimensionality reduction: 0.5350913233481492\n",
      "Accuracy of RandomForestClassifier'> without dimensionality reduction: 0.7626713479998506\n",
      "Accuracy of RandomForestClassifier'> with dimensionality reduction: 0.5468569080790349\n",
      "Accuracy of LogisticRegression'> without dimensionality reduction: 0.812385612370672\n",
      "Accuracy of LogisticRegression'> with dimensionality reduction: 0.5350913233481492\n",
      "Accuracy of SGDClassifier'> without dimensionality reduction: 0.796362006499085\n",
      "Accuracy of SGDClassifier'> with dimensionality reduction: 0.5350913233481492\n",
      "Accuracy of RandomForestClassifier'> without dimensionality reduction: 0.7576663056063945\n",
      "Accuracy of RandomForestClassifier'> with dimensionality reduction: 0.5257162066260785\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for model in models:\n",
    "    model_name = str(type(model.named_steps['classifier'])).split('.')[-1]\n",
    "    \n",
    "    if 'reducer' in model.named_steps:\n",
    "        acc_print = 'Accuracy of {} with dimensionality reduction: {}'\n",
    "    else:\n",
    "        acc_print = 'Accuracy of {} without dimensionality reduction: {}'\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores.append(accuracy)\n",
    "    print(acc_print.format(model_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more advanced - using embeddings not provided by sklearn\n",
    "\n",
    "Let's say you want to embed your text-data using a different embedding method, one not provided by scikit-learn. However, you also still want to be able to use those awesome `sklearn.Pipeline` objects. \n",
    "\n",
    "What can ya do???\n",
    "\n",
    "Wrap your own sklearn transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-437b715d805a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit dictionary to training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# create bag-of-words for training & testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         logger.info(\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \"\"\"\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# fit dictionary to training data\n",
    "dictionary = Dictionary(X_train)\n",
    "\n",
    "# create bag-of-words for training & testing data\n",
    "corpus = [dictionary.doc2bow(text) for text in X_train]\n",
    "corpus_test = [dictionary.doc2bow(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfIdfTransformer(id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           0\n",
       "title                0\n",
       "id                   0\n",
       "subreddit            0\n",
       "body             85847\n",
       "comment              1\n",
       "clean_comment        0\n",
       "label                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaSeqTransformer(id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfIdfTransformer(dictionary=None,\n",
       "         id2word=<gensim.corpora.dictionary.Dictionary object at 0x000002463519D208>,\n",
       "         normalize=True, pivot=None, slope=0.65, smartirs='ntc',\n",
       "         wglobal=<function df2idf at 0x00000246098F8C80>,\n",
       "         wlocal=<function identity at 0x000002460DA759D8>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcorp = tfidf.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62465\n",
      "62465\n"
     ]
    }
   ],
   "source": [
    "tc2 = [i for i in tcorp if len(i) > 0]\n",
    "print(len(tc2))\n",
    "print(len(y_train[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-3fce6c5ffd4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtc2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    746\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\",\n\u001b[1;32m--> 570\u001b[1;33m                          accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "sgd.fit(tc2, y_train[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I read somewhere that most people who think th...</td>\n",
       "      <td>[i, read, somewhere, that, most, people, who, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got Oscar Martinez... Michael am I gay?</td>\n",
       "      <td>[i, got, oscar, martinez, michael, am, i, gay]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That is correct.</td>\n",
       "      <td>[that, is, correct]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Am I the only one who took slight pride in get...</td>\n",
       "      <td>[am, i, the, only, one, who, took, slight, pri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You got: Creed Bratton\\nYou're very mysterious...</td>\n",
       "      <td>[you, got, creed, bratton, you're, very, myste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       title      id      subreddit body  \\\n",
       "0           0  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "1           1  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "2           2  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "3           3  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "4           4  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "\n",
       "                                             comment  \\\n",
       "0  I read somewhere that most people who think th...   \n",
       "1          I got Oscar Martinez... Michael am I gay?   \n",
       "2                                   That is correct.   \n",
       "3  Am I the only one who took slight pride in get...   \n",
       "4  You got: Creed Bratton\\nYou're very mysterious...   \n",
       "\n",
       "                                       clean_comment  label  \n",
       "0  [i, read, somewhere, that, most, people, who, ...      0  \n",
       "1     [i, got, oscar, martinez, michael, am, i, gay]      0  \n",
       "2                                [that, is, correct]      0  \n",
       "3  [am, i, the, only, one, who, took, slight, pri...      0  \n",
       "4  [you, got, creed, bratton, you're, very, myste...      0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
