{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note, this notebook uses `nltk`, which I don't have in the environment, so you'll have to install it yourself:\n",
    "\n",
    "e.g.;\n",
    "\n",
    "- `pip install nltk`\n",
    "- `python -m import nltk; nltk.download()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Your Own Transformers\n",
    "\n",
    "Sometimes you want to create pipelines for things not provided by scikit-learn. For example, you may want to use your own text-processing pipeline, or embedding methods from a different library (i.e. `gensim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Office df shape: (41467, 6)\n",
      "Overwatch df shape: (47774, 6)\n"
     ]
    }
   ],
   "source": [
    "# combine all daasets\n",
    "office_df = pd.read_csv('../data/dundermifflin.csv')\n",
    "print('Office df shape:', office_df.shape)\n",
    "\n",
    "overwatch_df = pd.read_csv('../data/overwatch.csv')\n",
    "print('Overwatch df shape:', overwatch_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89241, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I read somewhere that most people who think th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got Oscar Martinez... Michael am I gay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That is correct.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Am I the only one who took slight pride in get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Should I call you Jimothy?</td>\n",
       "      <td>ay2o5j</td>\n",
       "      <td>DunderMifflin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You got: Creed Bratton\\r\\nYou're very mysterio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       title      id      subreddit body  \\\n",
       "0           0  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "1           1  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "2           2  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "3           3  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "4           4  Should I call you Jimothy?  ay2o5j  DunderMifflin  NaN   \n",
       "\n",
       "                                             comment  \n",
       "0  I read somewhere that most people who think th...  \n",
       "1          I got Oscar Martinez... Michael am I gay?  \n",
       "2                                   That is correct.  \n",
       "3  Am I the only one who took slight pride in get...  \n",
       "4  You got: Creed Bratton\\r\\nYou're very mysterio...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine data into single DataFrame\n",
    "all_df = pd.concat([office_df, overwatch_df])\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = all_df['comment']\n",
    "y = all_df['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
    "\n",
    "# Unix System (mac/ubuntu/etc.)\n",
    "# nlp = spacy.load('en', disable=['ner', 'parser']) \n",
    "\n",
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our own text processor transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class TextProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom sklearn transformer to preprocess data.\n",
    "    Data fit to should be raw document (not split by tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, remove_stopwords=True, do_lemmatize=False, do_stem=False, stop_words=None, return_list=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        remove_stopwords: Boolean\n",
    "            Whether or not to remove stopwords.\n",
    "        do_lemmatize: Boolean\n",
    "            Whether or not to lemmatize tokens.\n",
    "        do_stem: Boolean\n",
    "            Whether or not to stem tokens.\n",
    "        stop_words: iterable\n",
    "            List of stop words, each word of type str.\n",
    "        return_list: Boolean\n",
    "            Whether or not to return list or raw document.\n",
    "            Some sklearn vectorizers prefer a raw document.\n",
    "        Note - if neither `do_lemmatize` nor `do_stem` are specified, the transform\n",
    "        method will preprocess using `fast_process`, which is much faster.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "   \n",
    "    def clean_token(self, token):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        c_token = re.sub(\"[^A-Za-z']+\", ' ', str(token))\n",
    "        # lower-case and strip whitespace\n",
    "        c_token = c_token.lower().strip()\n",
    "        # remove stopwords\n",
    "        if c_token in stop_words:\n",
    "            return ''\n",
    "        return c_token\n",
    "\n",
    "    def preprocess(self, document):\n",
    "        \"\"\"\n",
    "        Preprocessing method that stems and/or lemmatizes token\n",
    "        during a single pass through data.\n",
    "        \"\"\"\n",
    "        if not isinstance(document, str):\n",
    "            return ['']\n",
    "        clean_tokens = [\n",
    "            self.clean_token(token) \n",
    "            for token in document.split()\n",
    "        ]\n",
    "        # remove empty strings\n",
    "        cleaned_tokens = [com for com in clean_tokens \n",
    "                          if com != '']\n",
    "        return cleaned_tokens\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        `fit` method required for inclusion in sklearn pipeline object.\n",
    "        Returns data as is.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Preprocesses each document in `documents`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents: iterable\n",
    "            List of documents. Each document raw, untokenized text.\n",
    "        Returns\n",
    "        -------\n",
    "        iterable: list of lists. Each list contains the preprocessed\n",
    "            tokens for a document.\n",
    "        \"\"\"\n",
    "        document_list = []\n",
    "        for document in documents:\n",
    "            document_list.append(self.preprocess(document))\n",
    "        return document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextProcessor()\n",
    "tp.fit(X_train)\n",
    "cleaned_data = tp.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['welcome', 'reddit'], ['holy', 'shit', \"you're\", 'right', 'thought', 'looked', 'familiar', 'edit', 'oscar', 'people', 'earth'], ['ironic'], ['wait', 'toby', 'oscar', 'hockey']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom sklearn object wrapping a gensim object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GensimBOW(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom sklearn transformer to convert tokenized,\n",
    "    preprocessed data to bag-of-words representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_sparse_representation=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        use_sparse_representation: Boolean (default=False)\n",
    "            When True, a sparse representation of the array is returned.\n",
    "                Use this when feeding into a gensim model.\n",
    "            When False, the full array is returned.\n",
    "                Use this if feeding into sklearn estimator.\n",
    "        \"\"\"\n",
    "        self.id2word = None\n",
    "        self.use_sparse_representation = use_sparse_representation\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        \"\"\"\n",
    "        Creates map between words and their integer ids,\n",
    "        storing it as `self.id2word`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents: iterable\n",
    "            List of documents; each document a list of preprocessed tokens.\n",
    "        labels:\n",
    "            Optional list of same size as documents, specifying label for each document.\n",
    "        \"\"\"\n",
    "        from gensim.corpora.dictionary import Dictionary\n",
    "        self.id2word = Dictionary(documents)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Converts a collection of words to its bag-of-words representation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents: iterable\n",
    "            List of documents. Each document must be a list of tokens.\n",
    "        Returns\n",
    "        -------\n",
    "            generator: yields vectorized representation of each document.\n",
    "        \"\"\"\n",
    "        from gensim.matutils import sparse2full\n",
    "        if self.id2word is None:\n",
    "            raise AttributeError('Must have a fit id2word in order'\n",
    "                                 ' to call transform.')\n",
    "\n",
    "        def generator():\n",
    "            \"\"\"\n",
    "            Closure to mutate return type depending on value of `use_sparse_representation`.\n",
    "            \"\"\"\n",
    "            for document in documents:\n",
    "                docbow = self.id2word.doc2bow(document)\n",
    "                yield sparse2full(docbow, len(self.id2word))\n",
    "        return list(generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbow = GensimBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GensimBOW(use_sparse_representation=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbow.fit(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = gbow.transform(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding our user-defined classes into a sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mod = Pipeline([\n",
    "    ('preprocessor', TextProcessor()),\n",
    "    ('vectorizer', GensimBOW()),\n",
    "    ('classifier', SGDClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jared.wilber\\appdata\\local\\continuum\\anaconda3\\envs\\athena\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', TextProcessor(do_lemmatize=None, do_stem=None, remove_stopwords=None,\n",
       "       return_list=None, stop_words=None)), ('vectorizer', GensimBOW(use_sparse_representation=False)), ('classifier', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=Fal...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
